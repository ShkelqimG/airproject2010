#LyX 1.6.4 created this file. For more info see http://www.lyx.org/
\lyxformat 345
\begin_document
\begin_header
\textclass sigplanconf
\use_default_options true
\begin_modules
theorems-ams
theorems-ams-extended
theorems-sec
\end_modules
\language english
\inputencoding auto
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Title
Twitter sentiment analysis: Good or Bad
\end_layout

\begin_layout Author
Ravi Gupta and Ninghang Hu
\end_layout

\begin_layout Date
May 26, 2010
\end_layout

\begin_layout Abstract
The paper provided an approach for automatically classifying the sentiment
 of Twitter messages.
 These messages are classified as either positive or negative.
 This is useful for consumers who want to research the sentiment of products
 before purchase, or companies that want to monitor the public sentiment
 of their brands.
 Our training data consists of Twitter messages with emoticons, and they
 are used as noisy labels.
 This type of training data is abundantly available and can be obtained
 through automated ways.
\end_layout

\begin_layout Abstract
We made a test to choose the number of features for classification and we
 showed different results of machine learning algorithms including Naive
 Bayes, Support Vector Machine, Maximum Entropy, Decision Tree and Winnow2.
 
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
In present Internet era, we see a lot of Internet users interacting with
 each other and share information using 
\begin_inset Quotes eld
\end_inset

social media
\begin_inset Quotes erd
\end_inset

.
 Social media has various forms like messages, pictures or video etc, and
 these social media can be used in multiple ways.
 One of the most important way is to share our feeling about something.
 For example, a video, a blog review or even some images.
 This sentiment information is very useful for both the customers and the
 companies in different domains, such as brand management or the usage of
 a new product.
\end_layout

\begin_layout Standard
Twitter is one of the largest social website among them.
 It provides the users a way to send short messages within 140 characters
 which other users can read and reply with.
 This short message is known as a tweet.
 Twitter owns more than 100 million users and there are 1.5 million tweets
 posted per hour.
 The sentiment information of tweets is quite interesting because they are
 posted by individuals, and sometimes they show the user's personal attitude
 to certain events.
\end_layout

\begin_layout Standard
In this paper, our aim is to build a system that can classify positive and
 negative message by using supervised learning.
 During the training part, supervised learning usually requires hand-labeled
 training data.
 With a large range of topics on Twitter, it would be quite difficult to
 manually annotate all the labels.
 Our solution is to use the training data with emotions.
 This approach was introduced by 
\begin_inset CommandInset citation
LatexCommand citet
key "read2005using"

\end_inset

.
 The emoticons serve as noisy labels.
 For example, :) in a tweet indicates that the tweet contains positive sentiment.
 With the help of the Twitter API, it is easy to extract large amounts of
 tweets with emoticons in them.
 This is a significant improvement over the many hours it may otherwise
 take to hand-label training data.
 We run classifiers trained on emoticon data against a test set of tweets,
 which may or may not have emoticons in them.
\end_layout

\begin_layout Section
Background
\end_layout

\begin_layout Standard
Sentiment analysis
\begin_inset CommandInset citation
LatexCommand cite
key "pang2008opinion"

\end_inset

 is a research area in the subject of Natural Language Processing and Text
 Mining.
 Research work has been done in different kind of text like blogs, movie
 review data etc.
 But sentiment analysis of tweets shows significantly different features
 from blog's and review's sentiment analysis.
 
\end_layout

\begin_layout Subsection
Tweets
\end_layout

\begin_layout Standard
Tweets are very much different that other text formats like blogs, normal
 reviews or webpages.
 Some of the differences are shown as follows:
\end_layout

\begin_layout Itemize
Tweets are generally 1 or 2 sentences with maximum of 140 characters.
 Whereas blogs or other reviews are generally very much bigger in size.
\end_layout

\begin_layout Itemize
Tweets are informal compared to blogs or reviews which are formal.
\end_layout

\begin_layout Itemize
Tweets are less ambiguous due limited expressive nature inherited from the
 its size limit as compared to blogs or other texts in which two opposite
 thoughts can be expressed.
\end_layout

\begin_layout Itemize
Normally in larger text sentiment analysis we are focused in some specific
 domain whereas even tweets of a single user are widely exposed to multiple
 domains.
\end_layout

\begin_layout Standard
Due to the unique nature of tweets, they also add new challenges to the
 system.
\end_layout

\begin_layout Itemize
Tweets are full of slang or colloquialisms e.g.
 mKay (I am ok).
\end_layout

\begin_layout Itemize
Tweets have grammatical and spelling error which are unconventional error
 e.g.
 luuuuuuuvvvv, hellllloooo.
\end_layout

\begin_layout Itemize
Most tweets consist of link only.
\end_layout

\begin_layout Itemize
Also few text in a tweet due to its character limit of 140.
\end_layout

\begin_layout Section
Data Preparation
\end_layout

\begin_layout Standard
We are collecting tweets data set ourselves using the Twitter search result.
 We are using the noisy label method,emoticons based search, for data collection.
 Then we are processing the tweet dataset for training and testing.
 All these steps are explained in the following subsections.
\end_layout

\begin_layout Subsection
Twitter Scrapper
\end_layout

\begin_layout Standard
Twython is a set of Twitter APIs for python users.
 Its search API function can return a number of results for each query.
 Each result will provide not only the message of the tweet but also the
 relevant information.
 As our approach focus on the message itself, we only keep the text and
 tweet ID.
 The tweet ID will be used to eliminating duplicated terms in the preprocessing.
 
\end_layout

\begin_layout Standard
There are over 1,500,000 tweets are posted online every hour
\begin_inset Foot
status open

\begin_layout Plain Layout
www.tweespeed.com-TweeSpeed The Twitter Instant Speed Meter
\end_layout

\end_inset

.
 However, our approach only focused on the data with smile or sad emoticons
 which is approximately 1% of the whole tweet collection.
 The number of tweets we got per hour is 15,000.
 This was partly due to the limitation of Twitter API.
 As it noticed on Twitter, some users can be absent according to their privacy
 settings
\begin_inset Foot
status open

\begin_layout Plain Layout
http://help.twitter.com/forums/10713/entries/42646
\end_layout

\end_inset

.
 Thus, we asked the scrapper to wait a few seconds to avoid getting too
 much duplicated data.
 Besides, search rate is also limited, and we found that if the scrapper
 will result in errors if it searched too frequently.
 We found 10 seconds would be a good time span to avoid getting too much
 errors.
 
\end_layout

\begin_layout Standard
We also found that Twitter API support ambiguity search.
 For example, while we are searching 
\begin_inset Quotes eld
\end_inset

:)
\begin_inset Quotes erd
\end_inset

, the key word will be mapped into a set of similar emoticons, see Table
 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Mapping-of-Emoticons"

\end_inset

, and the search result will also include other smiley icons.
 Thus we only need to use :) to search tweets that include a smiley.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="7" columns="2">
<features>
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Positive
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Negative
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
:)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
:(
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
: )
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
: (
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
:-)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
:-(
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
=)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
=(
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
:D
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
:P
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
lol
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:Mapping-of-Emoticons"

\end_inset

Mapping of Emoticons
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We run the scrapper for approximately 40 hours and totally 384,397 tweets
 were collected(including 232,442 with positive emoticons and 151,955 with
 negative emoticons).
\end_layout

\begin_layout Subsection
Preprocessing
\begin_inset CommandInset citation
LatexCommand cite
key "manning-introduction"

\end_inset


\end_layout

\begin_layout Standard
Language used in tweets is quite different from formal language.
 As in tweets, most of the users prefer to use short cut words, emoticons
 and loop characters to express their ideas.
 Some users upload tweets by mobile phones and they are more likely to type
 at a high speed, which resulted in spelling errors.
 Also the usernames and the links should be properly handled as they should
 not affect the result of classification since they will not contribute
 to the sentiment of a tweet..
 As a result of all these features, preprocessing is quite crucial on reducing
 the side effect of casual language and the dimension of the feature space.
\end_layout

\begin_layout Enumerate
Duplicate Tweets:
\begin_inset Newline newline
\end_inset

We used the tweets with emoticons as our training data.
 As users are unlikely to add emoticon all the time and the limitation of
 Twitter API, only a small number of tweets can be obtained every hour.
 But our search frequency was set quite high to get as many tweets as possible,
 which resulted in retrieving lot of duplicates.
 We simply recorded the tweet ID while searching and remove all the duplicated
 tweets using the tweet ID.
\end_layout

\begin_layout Enumerate
Links, Emails and Usernames
\begin_inset Newline newline
\end_inset

Twitter users use links to share interesting websites.
 Those links is consisted of a sequence of words, and sometimes the words
 can bias the sentiment classification.
 For example, the tweet
\emph on

\begin_inset Newline newline
\end_inset

@ninghang http://www.happy.cn
\emph default

\begin_inset Newline newline
\end_inset

is classified into the positive category as the word 
\emph on
happy 
\emph default
has high probability of positive emotion.
 Besides, the length of a link is usually quite long, and it takes a lot
 of space and time to process.
 Similarly, a user named 
\emph on
Happy 
\emph default
or a email named 
\emph on
happy@gmail.com
\emph default
 can have a bad effect on the results as well.
 
\begin_inset Newline newline
\end_inset

Thus we used the words URL, EMAIL and USERNAME to replace all the links,
 emails and usernames in our database to eliminate the side effects.
 The regular expression pattern was shown in Algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:regular-expression-patterns"

\end_inset

.
 In our experiment, totally 198133 usernames, 19819 URLs and 319 email address
 were replaced.
 
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Description
URL ((?:http|https)://[a-z0-9
\backslash
/
\backslash
?=_#&%~-]+(
\backslash
.[a-z0-9
\backslash
/
\begin_inset Newline newline
\end_inset


\backslash
?=_#&%~-]+)+)|www(
\backslash
.[a-z0-9
\backslash
/
\backslash
?=_#&%~-]+){2,}
\end_layout

\begin_layout Description
EMAIL 
\backslash
w+@
\backslash
w+.[a-zA-Z]{2,4}
\end_layout

\begin_layout Description
USERNAME @([A-Za-z0-9_]+)
\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "alg:regular-expression-patterns"

\end_inset

regular expression patterns for preprocessing
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
Loop Characters
\begin_inset Newline newline
\end_inset

Loop characters are quite common in Twitter.
 For example, people usually say 
\begin_inset Quotes eld
\end_inset

LOOOOVVVVEEEE
\begin_inset Quotes erd
\end_inset

 to expressive their feelings.
 In common use, we found that there is hardly any character repeating consecutiv
ely more than two times in an English word.
 So we changed the word into 
\begin_inset Quotes eld
\end_inset

LOOVVEE
\begin_inset Quotes erd
\end_inset

, which made it easier for word correction.
\end_layout

\begin_layout Enumerate
Tweet include both smiley and frowny emoticons
\begin_inset Newline newline
\end_inset

As we searched the both smile and sad emoticons separately, some tweets
 with both negative and positive emotions were present on both dataset.
 We removed these tweets from our training set.
 For example, 
\begin_inset Quotes eld
\end_inset


\emph on
USERNAME lol he wasn't yesterday but we're alright now :) aaghh i have to
 go to work when its nice and sunnyy..
 typical! :-(
\emph default

\begin_inset Quotes eld
\end_inset

.
\end_layout

\begin_layout Enumerate
Punctuation
\begin_inset Newline newline
\end_inset

Punctuations at the beginning or the end of a sentence can make a lot of
 new words as well.
 For example, at the end of our last example, the word 
\begin_inset Quotes eld
\end_inset


\emph on
typical!
\emph default

\begin_inset Quotes erd
\end_inset

 would cause noise if it was added to the features.
 So punctuations are removed.
\end_layout

\begin_layout Enumerate
:P good or bad
\begin_inset Newline newline
\end_inset

In the search result of Twitter API, the emoticon :P is considered as a
 unhappy emotion.
 However, we found that most of the time :P indicates a positive status.
 Thus in our experiment, we removed all the tweets containing :P emoticon.
 
\end_layout

\begin_layout Subsection
Test set
\end_layout

\begin_layout Standard
For the test set, we used a manually annotated dataset
\begin_inset CommandInset citation
LatexCommand citet
key "go-twitter"

\end_inset

.
 Test set contains 183 tweets which were selected from different topics.
 All the tweets have their labels for which class they belong to.
\end_layout

\begin_layout Section
Classifiers
\end_layout

\begin_layout Standard
In our classification task, we extracted three different kinds of feature
 set: unigram, bigram and the combination of unigram and bigram.
 The features were fed into the classifiers.
 Five classifiers were experimented including Key Word, Naive Bayes, SVM,
 Maximum entropy, Decision Tree and Winnow2.
\end_layout

\begin_layout Subsection
Key Words Classifier
\end_layout

\begin_layout Standard
Key words classifier is a straight forward way of sentiment classification.
 As a baseline, we used a list of positive and negative words, and counted
 the number of those words in each tweet.
 If the tweet has more positive words than negative ones, it turns out to
 be positive, and vise verse.
 If there is a tie, it was classified to positive class since there are
 more positive tweets in the test set.
 The key words list is publicly available on Twittratr
\begin_inset Foot
status open

\begin_layout Plain Layout
www.twitrratr.com
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Subsection
Naive Bayes Classifier
\end_layout

\begin_layout Standard
Naive Bayes
\begin_inset CommandInset citation
LatexCommand cite
key "parikh,keerthi2003asymptotic"

\end_inset

 classifier uses Bayes rule of probability to compute the posterior probability
 of the class given the data with a very strong assumption.
 The strong assumption which this classifier makes is that the features
 are independent of each other.
\end_layout

\begin_layout Standard
The posterior probability of a class 
\begin_inset Formula $C_{k}$
\end_inset

 given the tweet 
\begin_inset Formula $\mathbf{x}$
\end_inset

is given by: 
\begin_inset Formula \[
p(C_{k}|\mathbf{x})=\frac{p(\mathbf{x}|C_{k})p(C_{k})}{\sum_{i}p(\mathbf{x}|C_{i})p(C_{i})}\]

\end_inset

 Tweet 
\begin_inset Formula $\mathbf{x}$
\end_inset

is a 
\begin_inset Formula $d$
\end_inset

 dimensional vector of features 
\begin_inset Formula $=\{\mathbf{f}_{1},\mathbf{f}_{2},...,\mathbf{f}_{d}\}$
\end_inset

.
 And based on the assumption that the features are independent, the likelihood
 is given as following:
\end_layout

\begin_layout Standard
\begin_inset Formula \[
p(\mathbf{x}|C_{k})=\prod_{i=1}^{d}p(\mathbf{f}_{i}|C_{k})\]

\end_inset

Three different models for sentiment classification of tweets were implemented
 using python programming language.
 The implementation of models is very general and simply varying some of
 the parameters of the program different models can also be created.
 But we restricted to the following models because of unique nature of tweet:
\end_layout

\begin_layout Enumerate
Unigram Model: In this model each word 
\begin_inset Formula $\mathbf{w}_{n}$
\end_inset

 in a tweet 
\begin_inset Formula $\mathbf{x}$
\end_inset

is a feature 
\begin_inset Formula $\mathbf{f}_{i}$
\end_inset

.
 So 
\begin_inset Formula $\mathbf{\mathbf{f}}_{i}$
\end_inset

is a feature then its likelihood is given as following:
\begin_inset Formula \[
p(\mathbf{f}_{i}|C)=\frac{count(\mathbf{\mathbf{f}}_{i},C)}{count(C)}\]

\end_inset

 where 
\begin_inset Formula $count(\mathbf{\mathbf{f}}_{i},C)=$
\end_inset

no.
 of times the feature 
\begin_inset Formula $\mathbf{f}_{i}$
\end_inset

(or the word
\begin_inset Formula $\mathbf{w}_{n}$
\end_inset

) comes in the class 
\begin_inset Formula $C$
\end_inset

 and 
\begin_inset Formula $count(C)=$
\end_inset

total no.
 of features (or words) in class 
\begin_inset Formula $C$
\end_inset


\end_layout

\begin_layout Enumerate
Bigram Model: In this model two consecutive words (
\begin_inset Formula $\mathbf{w}_{n}\mathbf{w}_{n+1}$
\end_inset

) in a tweet 
\begin_inset Formula $\mathbf{x}$
\end_inset

 is a feature 
\begin_inset Formula $\mathbf{f}_{i}$
\end_inset

.
 And its likelihood is given similar to above, with the modification that:
 
\begin_inset Formula $count(\mathbf{\mathbf{f}}_{i},C)=$
\end_inset

no.
 of times the feature 
\begin_inset Formula $\mathbf{f}_{i}$
\end_inset

(or the consecutive word pair
\begin_inset Formula $\mathbf{w}_{n}\mathbf{w}_{n+1}$
\end_inset

) comes in the class 
\begin_inset Formula $C$
\end_inset

 and 
\begin_inset Formula $count(C)=$
\end_inset

total no.
 of features (or consecutive word pairs) in class 
\begin_inset Formula $C$
\end_inset


\end_layout

\begin_layout Enumerate
Linear combination of Unigram and Bigram Model: This model is linear combination
 of the above two models.
 So we have:
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
 
\begin_inset Formula \[
p(C_{k}|\mathbf{x})=\alpha p_{1}(C_{k}|\mathbf{x})+(1-\alpha)p_{2}(C_{k}|\mathbf{x})\]

\end_inset

 where 
\begin_inset Formula $\alpha\mathbf{\mathtt{\epsilon}}(0,1)$
\end_inset

is parameter calculated from the experiment and 
\begin_inset Formula $p_{1}$
\end_inset

and 
\begin_inset Formula $p_{2}$
\end_inset

values are calculated using the above model 1 and 2 respectively.
\end_layout

\begin_layout Standard
Laplace add one smoothing is used for unseen features.
\end_layout

\begin_layout Subsection
Support Vector Machine
\end_layout

\begin_layout Standard
Support Vector Machine(SVM) is a popular technique in text classification.
 Many SVM packages are publicly available online.
 LibSVM
\begin_inset CommandInset citation
LatexCommand citet
key "CC01a"

\end_inset

 is an integrated software for support vector classification, (C-SVC, nu-SVC),
 regression (epsilon-SVR, nu-SVR) and distribution estimation (one-class
 SVM).
 It also provide programming interface for Python as it has provided interfaces
 for Python, which is used most of the time in our implementation.
\end_layout

\begin_layout Subsection
Maximum Entropy Classifier
\end_layout

\begin_layout Standard
Maximum Entropy
\begin_inset CommandInset citation
LatexCommand citet
key "parikh,nigam1999using"

\end_inset

 Modeling also known as multinomial logistic regression.
 This method belongs to the family of classifiers known as the exponential
 or log-linear classifiers.
 Given the inputs which are tweets here we select features and for each
 feature we have some weight for each class and this we use to calculate
 the maximum likelihood of the tweet class.
 The likelihood is given as following:
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
\begin_inset Formula \[
P(C_{k}|\mathbf{x})=\frac{1}{Z(\mathbf{x})}e^{\sum_{i}\lambda_{i,C_{k}}f_{i,C_{k}}(\mathbf{x},C_{k})}\]

\end_inset

 We used MALLET
\begin_inset Foot
status open

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
http://mallet.cs.umass.edu
\end_layout

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "mallet"

\end_inset

 package for this classifier.
 MALLET is a Java based package which also contains MaxEnt classifier.
\end_layout

\begin_layout Subsection
Decision Tree Learning Classifier
\end_layout

\begin_layout Standard
Decision tree learning, used in data mining and machine learning, uses a
 decision tree as a predictive model which maps observations about an item
 to conclusions about the item's target value.
 More descriptive names for such tree models are classification trees or
 regression trees.
 In these tree structures, leaves represent classifications and branches
 represent conjunctions of features that lead to those classifications.
\end_layout

\begin_layout Standard
We used Malledt package for this classifier.
\end_layout

\begin_layout Subsection
Winnow2 Classifier
\end_layout

\begin_layout Standard
Winnow2 classifier is very similar to the perceptron algorithm.
 However, the perceptron algorithm uses an additive weight-update scheme,
 but winnow uses a multiplicative weight-update scheme that allows it to
 perform much better when many dimensions are irrelevant (hence its name).
 It is not a sophisticated algorithm but it scales well to high-dimensional
 spaces.
 During training, winnow is shown a sequence of positive and negative examples.
 From these it learns a decision hyperplane.
 It can also be used in the online learning setting, where the learning
 phase is not separated from the training phase.
\end_layout

\begin_layout Standard
We are using Winnow2 as classifier and not Winnow1.
\end_layout

\begin_layout Standard
In Winnow2 :
\end_layout

\begin_layout Standard
The instance space is 
\begin_inset Formula $X={0,1}^{n}$
\end_inset

.
 The algorithm maintains non-negative weights 
\begin_inset Formula $w_{i}$
\end_inset

 for which are initially set to 
\begin_inset Formula $1$
\end_inset

.
 When the learner is given an example 
\begin_inset Formula $(x_{1},..,x_{n})$
\end_inset

, the learner follows the following prediction rule:
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $\sum_{i=1}^{n}w_{i}x_{i}>\Theta$
\end_inset

, then it predicts 1(positive tweet)
\end_layout

\begin_layout Itemize
Otherwise it predicts 0(negative tweet)
\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\Theta$
\end_inset

is threshold.
\end_layout

\begin_layout Standard
The update rules are following:
\end_layout

\begin_layout Itemize
If an example is correctly classified, do nothing.
\end_layout

\begin_layout Itemize
If an example is predicted to be 1 (positive) but the correct result was
 0 (negative), all of the weights involved in the mistake are multiplied
 by 
\begin_inset Formula $\frac{1}{\alpha}$
\end_inset

(demotion step).
\end_layout

\begin_layout Itemize
If an example is predicted to be 0 (negative) but the correct result was
 1 (positive), all of the weights involved in the mistake are multiplied
 by 
\begin_inset Formula $\alpha$
\end_inset

 (promotion step).
\end_layout

\begin_layout Standard
A good value for 
\begin_inset Formula $\alpha$
\end_inset

is 2.
 We used Malledt package for this classifier.
\end_layout

\begin_layout Section
Evaluation
\end_layout

\begin_layout Subsection
Feature Selection
\end_layout

\begin_layout Standard
After preprocessing, we made a dictionary for all the words with their occurrenc
es.
 Each word in the dictionary was a feature in the classification.
 
\end_layout

\begin_layout Standard
However, there were 30696 words in the dictionary, and it was too large
 for SVM classifier.
 We tested on the whole words and the libSVM took about 7 hours to get the
 result.
 Besides, about 17010 words occur only once in the training set(see 
\begin_inset CommandInset ref
LatexCommand prettyref
reference "fig:frequency-of-feature"

\end_inset

).
 Most of them are numbers(eg.
 
\begin_inset Quotes eld
\end_inset

520
\begin_inset Quotes erd
\end_inset

), misspelled words(eg.
 
\begin_inset Quotes eld
\end_inset

nive
\begin_inset Quotes erd
\end_inset

) or errors caused by preprocessing(eg.
 
\begin_inset Quotes eld
\end_inset

I'm
\begin_inset Quotes erd
\end_inset

 was converted to 
\begin_inset Quotes eld
\end_inset

I
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

m
\begin_inset Quotes erd
\end_inset

).
 These words and other low frequency words were considered as noise and
 were removed before training.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename featureSele.eps
	width 39page%

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:frequency-of-feature"

\end_inset

frequency of feature words
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
High frequency words(stop words) were noise in our approach as well.
 But we found some of the high frequency words has sentiment information
 which is quite important for our classification, such as 
\begin_inset Quotes eld
\end_inset

better
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

like
\begin_inset Quotes erd
\end_inset

.
 Further, the number of high frequency words was very small compared with
 low frequency ones.
 Thus we made no change to these words in this paper.
 
\end_layout

\begin_layout Standard
We used libSVM and test on different number of features(a bag of words in
 our case).
 To avoid bias with our formal test set, we made a extra small test set
 manually for this part.
 Words in the dictionary were firstly ordered from high frequency to low
 frequency, and lower frequency words were added into the features after
 each process of classification.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename accuracy.eps
	width 39page%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:accuracy-features"

\end_inset

unigram feature selection with default parameters
\end_layout

\end_inset


\end_layout

\end_inset

 
\begin_inset CommandInset ref
LatexCommand prettyref
reference "fig:accuracy-features"

\end_inset

 , the accuracy increase dramatically as the number of features growing
 at the beginning.
 But after a peak at 80.9% with 3494 features, the accuracy is stable around
 79.5%.
 
\end_layout

\begin_layout Subsection
SVM Kernels and Parameters
\end_layout

\begin_layout Standard
There are four common kernels in libSVM, and it would take a long time to
 test all of them with different parameters.
 According to a practical guide written by the authors of libSVM, if the
 number of features is large, one may not need to map data to a higher dimension
 space.
 In other words, the nonlinear mapping does not improve the performance
\begin_inset CommandInset citation
LatexCommand citet
key "hsu2003practical"

\end_inset

.
 In this paper, we used linear kernel since we have more than 4000 features.
 
\end_layout

\begin_layout Standard
There is another advantage of using linear kernels.
 We only need to search the parameter C rather than both 
\begin_inset Formula $C$
\end_inset

 and 
\begin_inset Formula $\gamma$
\end_inset

.
 Although for most non-linear classifications, Radius Basis Function(RBF)
 kernel is the first choice of SVM and the linear kernel is a special case
 of 
\begin_inset CommandInset citation
LatexCommand citet
key "keerthi2003asymptotic"

\end_inset

since the linear kernel with a penalty parameter 
\emph on
C
\emph default
 has the same performance as the RBF kernel with some parameters 
\begin_inset Formula $C$
\end_inset

 and 
\begin_inset Formula $\gamma$
\end_inset

, the case is true only we got the best 
\begin_inset Formula $C$
\end_inset

 and 
\begin_inset Formula $\gamma$
\end_inset

 pair after searching the entire parameter space.
 Thus with linear kernel, the searching time is reduced from 
\begin_inset Formula $N_{c}\times N_{\gamma}$
\end_inset

 to 
\begin_inset Formula $N_{c}$
\end_inset

, where 
\begin_inset Formula $N_{c}$
\end_inset

 and 
\begin_inset Formula $N_{\gamma}$
\end_inset

 are the corresponding searching times in the classification.
\end_layout

\begin_layout Standard
After selected the linear kernel, we made a grid search on the parameters
 
\emph on

\begin_inset Formula $C$
\end_inset

,
\emph default
 and
\emph on
 
\emph default
the searching grid was 
\begin_inset Formula $C=2^{-5},2^{-3},...,2^{15}$
\end_inset

.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename SVMparamC.eps
	width 39page%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:choose-parameter-C"

\end_inset

parameter C with different SVM kernel
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We tried linear kernel SVM with a grid search of 
\begin_inset Formula $C$
\end_inset

.
 The result(red curve) is shown in 
\begin_inset CommandInset ref
LatexCommand prettyref
reference "fig:choose-parameter-C"

\end_inset

.
 We found that 
\begin_inset Formula $C=2^{-3}$
\end_inset

 is a good choice with the highest accuracy over 80.8%.
 But when the 
\begin_inset Formula $C$
\end_inset

 grows to 
\begin_inset Formula $2^{2}$
\end_inset

, it takes more than 20 hours to finish the training.
 The black curve shows the performance of RBF kernel.
 In this experiment, the default 
\begin_inset Formula $\gamma$
\end_inset

 in libSVM was used and we can see that RBF kernel is not better than linear
 kernel unless the best pair of 
\begin_inset Formula $\gamma$
\end_inset

 and 
\begin_inset Formula $C$
\end_inset

 is used.
\begin_inset CommandInset citation
LatexCommand cite
key "keerthi2003asymptotic"

\end_inset


\end_layout

\begin_layout Subsection
Classification Results
\end_layout

\begin_layout Standard
For MaxEnt we used the MALLET tool as we found it difficult to implement
 it ourselves.
 We used the command line system called MALLET.
 We used random 90% as training data and 10% as testing set.
 There were 25 number of trials in the MaxEnt classification.
 Using the command line functions we obtained F1 score, Accuracy and confusion
 matrix for each trail.
 F1 score for both positive and negative class and accuracy from the cross
 validation setup of MaxEnt classifier are shown in Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:MaxEnt-Result"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="4">
<features>
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
F1 Positive Tweet
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
F1 Negative Tweet
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Accuracy
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Worst Trial
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.7875
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.7942
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
78.98%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Best Trial
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.8693
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.8781
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
86.90%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Avg.
 Trial
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.8394
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.8415 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
83.99%
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:MaxEnt-Result"

\end_inset

MaxEnt Result
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
We also used Decision tree experiment in a cross validation setup using
 the MALLET toolkit.
 The experimental parameters used are 90% data for training, 10% testing
 and 25 no.
 of trials.
 F1 score for Decision Tree classifier is shown in Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Decision-Tree-Result"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="4">
<features>
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
F1 Positive Tweet
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
F1 Negative Tweet
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Accuracy
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Worst Trial
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.5873
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.8038
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
64.73%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Best Trial
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.6219
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.8192
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
67.86%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Avg.
 Trial
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.6118
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.8076 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
66.53%
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:Decision-Tree-Result"

\end_inset

Decision Tree Result
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
For Winnow classifier, the experimental parameters used are same as above.
 F1 score for both positive and negative class and accuracy are shown in
 Table
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Winnow2-Result"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="4">
<features>
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
F1 Positive Tweet
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
F1 Negative Tweet
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Accuracy
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Worst Trial
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.5921
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.8992
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
68.59%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Best Trial
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.7642
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.6535
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
75.08%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Avg.
 Trial
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.7209
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.7517
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
72.98%
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:Winnow2-Result"

\end_inset

Winnow2 Result
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
From Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Accuracy-of-various"

\end_inset

 we can see the Key words classifier did very well although it was the most
 straight forward way of classification.
 Maximum Entropy Classifier had the best performance in unigram features.
 The accuracy was creased by 3% compared to the SVM classifier.
 However, when we turned to try the bigrams, the accuracy was quite low.
 It was due to the bigram data is quite sparse in our case.
 In the combination of unigram and bigram features, the performance was
 not as good as we expected.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="7" columns="4">
<features>
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Unigram
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Bigram
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Unigram+Bigram
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Key Words
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
70.58%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
N/A
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
N/A
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Naive Bayes
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
76.5%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
69.4%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
N/A
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
SVM
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
80.87%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
66.12%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
65.03%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
MaxEnt
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
83.99%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
N/A
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
N/A
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Decision Tree
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
66.53%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
N/A
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
N/A
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Winnow2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
72.98%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
N/A
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
N/A
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:Accuracy-of-various"

\end_inset

Accuracy of various Classifiers
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
Sentiment analysis is a interesting and challenging task.
 We found that Key words can be combined with the unigram features.
 The accuracy of key words classification is over 71%, which is really high
 considering such a naive approach.
 This should partly due to the selected key words earns high weights in
 the sentiment analysis.
 In order to reduce the dimension of feature, we eliminate some words with
 lower frequency in our training set.
 At the same time, some key words with lower occurrences in our training
 set are ignored as well although they are quite significant for sentiment
 analysis.
 In our future work, we will add key words to the features after feature
 selection.
 
\end_layout

\begin_layout Standard
As we looked into the features, we found that there are still quite a lot
 of noise in it.
 In our future research, we would like to build a vocabulary from our daily
 used words, and then implement word correction on the training and test
 date based on the vocabulary.
 
\end_layout

\begin_layout Standard
Emoticons are quite useful in classification.
 It can be treated as noisy labeled data which makes life quite easier.
 In the future, we would like to see how the experiment works with correctly
 manually annotated data.
\end_layout

\begin_layout Standard
For MaxEnt, Decision Tree and Winnow classifiers, features selection were
 automatic from the MALLET tool, we would like to do experiment by our own
 implement of these classifiers so that we can see effects of different
 features selection.
\end_layout

\begin_layout Standard
Trying the similar approach on different languages is also an interesting
 topic for us, such as sentiment analysis on Indian or Chinese in our case.
 In China, the first Microblog website has just started since last September,
 and there are a considerable number of Twitter like platforms joining the
 developing market.
 Although there was supposed to be large differences in the preprocessing
 level, we thing the selection of features and classification can be implement
 in a similar way.
 
\end_layout

\begin_layout Standard
Moreover, in this paper, we did not handle the neutral class(non-positive
 and non-negative).
 We will try Multi-class machine learning approach in the future.
 For the neutral class, the a headline of news or a sentence from Wikipedia
 would be a good source of training data.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "TwitterSenti"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
